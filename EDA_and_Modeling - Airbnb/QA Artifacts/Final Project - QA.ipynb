{"cells":[{"cell_type":"code","source":["pip install nltk"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccb9fa24-0783-48bb-9f50-0150d4ecd604"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nRequirement already satisfied: nltk in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7483b0e5-94d2-4018-8517-8a0a827b7d2e/lib/python3.7/site-packages (3.5)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.7/site-packages (from nltk) (0.14.1)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7483b0e5-94d2-4018-8517-8a0a827b7d2e/lib/python3.7/site-packages (from nltk) (4.54.1)\nRequirement already satisfied: regex in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7483b0e5-94d2-4018-8517-8a0a827b7d2e/lib/python3.7/site-packages (from nltk) (2020.11.13)\nRequirement already satisfied: click in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7483b0e5-94d2-4018-8517-8a0a827b7d2e/lib/python3.7/site-packages (from nltk) (7.1.2)\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nRequirement already satisfied: nltk in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7483b0e5-94d2-4018-8517-8a0a827b7d2e/lib/python3.7/site-packages (3.5)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.7/site-packages (from nltk) (0.14.1)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7483b0e5-94d2-4018-8517-8a0a827b7d2e/lib/python3.7/site-packages (from nltk) (4.54.1)\nRequirement already satisfied: regex in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7483b0e5-94d2-4018-8517-8a0a827b7d2e/lib/python3.7/site-packages (from nltk) (2020.11.13)\nRequirement already satisfied: click in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7483b0e5-94d2-4018-8517-8a0a827b7d2e/lib/python3.7/site-packages (from nltk) (7.1.2)\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Quality Assurance\n\nThe main objective here is to ensure that all transformations work perfectly. All the pipeline stages here are the same found on the Data Engineering notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"590e2342-f949-489d-bf45-6d18e796cfb8"}}},{"cell_type":"code","source":["from pyspark.ml.pipeline import Transformer\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import log10\nfrom pyspark.sql.functions import col, lower, regexp_replace, split\nfrom pyspark.sql.types import *\nfrom nltk.stem.porter import *\nfrom pyspark.sql import functions as f\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import avg, when\nglobal total_lost\n\nclass MeanImputation(Transformer):\n    def __init__(self, inputCols, outputCol):\n        self.inputCols = inputCols\n        self.outputCol = outputCol \n    def this():\n        this(Identifiable.uid(\"imputer\"))\n    def copy(extra):\n        defaultCopy(extra)\n    def _transform(self, df):\n        # operating in the speficied group of input columns\n        w = Window().partitionBy(self.inputCols)\n        #related to the data integrity specification\n        #print(\"------------- Percentage Recovered With Mean Imputation - \" + self.outputCol + \":\", df.filter((data[self.outputCol] == \"\") | df[self.outputCol].isNull() | isnan(df[self.outputCol])).count()/data.count()*100, \"% -----------------\")\n        # returns the new windowed average values when there is a null value on the column\n        return df.withColumn(self.outputCol,when(col(self.outputCol).isNull(),avg(col(self.outputCol)).over(w)).otherwise(col(self.outputCol)))\n      \nclass DropRows(Transformer):\n    def __init__(self, inputCols):\n        self.inputCols = inputCols\n    def this():\n        this(Identifiable.uid(\"Dropper_rows\"))\n    def copy(extra):\n        defaultCopy(extra)\n    def _transform(self, df):\n        # related to the data integrity specification\n        #print(\"------------- Percentage Lost in Drop Rows Stage:\", (df.na.drop(subset=self.inputCols).count())/df.count(), \"%-----------------\")\n        # returns a dataframe where there are not rows with null values in the specified columns\n        return df.na.drop(subset=self.inputCols)\n      \nclass LogTransform(Transformer):\n    def __init__(self, inputCol, outputCol):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n    def this():\n        this(Identifiable.uid(\"log_transform\"))\n    def copy(extra):\n        defaultCopy(extra)\n    def _transform(self, df):\n        # returns a dataframe with a log transformed input column as the specified output column\n        return df.withColumn(self.outputCol, log10((self.inputCol)))\n      \nclass CleanText(Transformer):\n    def __init__(self, inputCol, outputCol):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n    def this():\n        this(Identifiable.uid(\"clean_text\"))\n    def copy(extra):\n        defaultCopy(extra)\n    def _transform(self, df):\n        def clean_text(c):\n          # removes all these characters\n          c = lower(c)\n          c = regexp_replace(c, \"^rt \", \"\")\n          c = regexp_replace(c, \"(https?\\://)\\S+\", \"\")\n          c = regexp_replace(c, \"[^a-zA-Z0-9\\\\s]\", \"\")\n          return c\n        # applies the specified function above to the input column\n        return df.withColumn((self.outputCol), clean_text((self.inputCol)))\n  \nclass StemmerTransform(Transformer):\n    def __init__(self, inputCol, outputCol):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n    def this():\n        this(Identifiable.uid(\"clean_text\"))\n    def copy(extra):\n        defaultCopy(extra)\n    def _transform(self, df):\n        def stem(in_vec):\n          out_vec = []\n          stemmer = PorterStemmer()\n          for t in in_vec:\n              t_stem = stemmer.stem(t)\n              if len(t_stem) > 2:\n                  out_vec.append(t_stem)       \n          return out_vec\n        stemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n        return df.withColumn((self.outputCol), stemmer_udf(col(self.inputCol)))\n  \nclass CleanOutliers(Transformer):\n    def __init__(self, inputCol, outputCol, threshold):\n        self.inputCol = inputCol\n        self.outputCol = outputCol\n        self.threshold = threshold\n    def this():\n        this(Identifiable.uid(\"clean_outliers\"))\n    def copy(extra):\n        defaultCopy(extra)\n    def _transform(self, df):\n        return df.where(f.col(self.inputCol) < self.threshold)              \n      \n      \nclass EnsuretestSetQuality(Transformer):\n    def __init__(self, df_comparative, inputCols):\n        self.inputCols = inputCols\n        self.df_comparative = df_comparative\n    def this():\n        this(Identifiable.uid(\"testSetQual\"))\n    def copy(extra):\n        defaultCopy(extra)\n    def _transform(self, df):\n        for col in self.inputCols:\n          newCategorical = set(list(self.df_comparative.toPandas()[col])) - set(list(df.toPandas()[col]))\n          for cat in newCategorical:\n            df =  df = (df.filter(df[col] != cat))\n          \n        return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27828428-51d4-442a-947c-f3992e9b3135"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Unit Tests\n\nWe are unit testing the following customized pipeline stages:\n* LogTransform \n* MeanImputation\n* DropRows\n* CleanOutliers\n* CleanText\n* testSetQuality\n\nAll the testes were inherited from the unittest class."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4698dd58-0915-4a9b-8798-a242dd15ec5b"}}},{"cell_type":"code","source":["import unittest\n\nclass Tests(unittest.TestCase):\n  \n  def testLogTransform(self):\n    # Calling the class constructor\n    lt = LogTransform(inputCol='price',outputCol='priceLog')\n    # specifying the columns names and values\n    columns = [\"price\",\"any\"]\n    price_values = [(2,3),(3,2)]\n    # creating the df\n    df = sqlContext.createDataFrame(price_values,columns)\n    temp = lt.transform(df).toPandas()\n    # getting the column names\n    result1 = list(temp.columns)\n    # getting the column values\n    values = (temp.priceLog.values).tolist()\n    result2 = []\n    for i in range(len(values)):\n      # rounding values \n      result2.append(round(values[i],2))\n    # expected transformed df column names\n    expected1 = ['price','any','priceLog']\n    # expected transformed df values for d priceLog column\n    expected2 = [0.30, 0.48]\n    # test if this elements are equal\n    self.assertEqual(result1, expected1)\n    self.assertEqual(result2, expected2)\n    \n    \n  def testMeanImputation(self):\n    mi = MeanImputation(inputCols='city', outputCol='price')\n    #define the df\n    columns = ['city','price']\n    price = [('NY', 10), ('CG',10), ('SP',15),('SP',20),('CG',None),('NY',None),('CG',15),('SP',None)]\n    df = sqlContext.createDataFrame(price,columns)\n    temp = mi.transform(df).toPandas()\n    # column names of the transformed df\n    result1 = list(temp.columns)\n    expected1 = ['city', 'price']\n    self.assertEqual(result1, expected1)\n    # values of the transformed df\n    result2 = list(temp.price.values)\n    expected2 = [10, 12.5, 15, 10, 10, 15, 20, 17.5]\n    self.assertEqual(result2,expected2)\n  \n  def testDropRows(self):\n    dr = DropRows(inputCols=['room_type','availability_365'])\n    columns = ['room_type','availability_365']\n    values = [('private room',300), (None,365),('full', None),('shared', 120)]\n    df = sqlContext.createDataFrame(values,columns)\n    temp = dr.transform(df).toPandas()\n    result1 = list(temp.columns)\n    expected1 = ['room_type', 'availability_365']\n    result2 = list(temp.room_type.values)\n    expected2 = ['private room', 'shared']\n    result3 = list(temp.availability_365.values)\n    expected3 = [300,120]\n    self.assertEqual(result1,expected1)\n    self.assertEqual(result2,expected2)\n    self.assertEqual(result3,expected3)\n    \n    \n  def testCleanOutliers(self):\n    dr = CleanOutliers(inputCol='price', outputCol='price', threshold = 300)\n    columns = ['room_type','price']\n    values = [('private room',299), ('private room',300),('full', 305),('shared', 120)]\n    df = sqlContext.createDataFrame(values,columns)\n    temp = dr.transform(df).toPandas()\n    result1 = list(temp.columns)\n    expected1 = ['room_type', 'price']\n    result2 = list(temp.price.values)\n    expected2 = [299,120]\n    self.assertEqual(result1,expected1)\n    self.assertEqual(result2,expected2)\n  \n  def testcleanText(self):\n    ct = CleanText(inputCol='room_type', outputCol='room_type_clean')\n    columns = ['room_type','price']\n    values = [('p$rivate \\ room',299), ('?:// private room ',300),('full', 305),('sh^ared', 120)]\n    df = sqlContext.createDataFrame(values,columns)\n    temp = ct.transform(df).toPandas()\n    result1 = list(temp.columns)\n    expected1 = ['room_type', 'price', 'room_type_clean']\n    result2 = list(temp.room_type_clean)\n    expected2 = ['private  room',' private room ','full', 'shared']\n    self.assertEqual(result1,expected1)\n    self.assertEqual(result2,expected2)\n    \n  def testEnsuretestSetQuality(self):\n    # defining the result df\n    columns = ['neighbourhood_group','neighbourhood', 'room_type']\n    values = [('Manhattan','Bronx','Shared')]\n    df_train = sqlContext.createDataFrame(values,columns)\n    # defining the test df\n    columns = ['neighbourhood_group','neighbourhood', 'room_type']\n    values = [('Manhattan','Bronx','Shared'), ('Staten','Zepa', 'Private'),('Manhattan','Bronx','Private'),('Queens','Bronx','Full')]\n    df = sqlContext.createDataFrame(values,columns)\n    tq = EnsuretestSetQuality(df_comparative = df, inputCols=['neighbourhood_group','neighbourhood', 'room_type'])\n    result1 = (tq.transform(df_train).toPandas()).iloc[0].values.tolist()\n    expected1 = ['Manhattan','Bronx','Shared']\n    self.assertEqual(result1,expected1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"470f47ae-17e4-4bd4-bc94-8b17893e2480"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["As one can see, all unit tests for the custom pipeline stages were succesfull"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0b72dcb-abc9-4d99-ad34-f35a0e49630b"}}},{"cell_type":"code","source":["suite = unittest.TestLoader().loadTestsFromTestCase(Tests)\nrunner = unittest.TextTestRunner(verbosity=2)\nrunner.run(suite)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92d98ef7-4de3-43d9-80eb-e6ae599cfacc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">testCleanOutliers (__main__.Tests) ... ok\ntestDropRows (__main__.Tests) ... ok\ntestEnsuretestSetQuality (__main__.Tests) ... ok\ntestLogTransform (__main__.Tests) ... ok\ntestMeanImputation (__main__.Tests) ... ok\ntestcleanText (__main__.Tests) ... ok\n\n----------------------------------------------------------------------\nRan 6 tests in 3.169s\n\nOK\nOut[6]: &lt;unittest.runner.TextTestResult run=6 errors=0 failures=0&gt;</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">testCleanOutliers (__main__.Tests) ... ok\ntestDropRows (__main__.Tests) ... ok\ntestEnsuretestSetQuality (__main__.Tests) ... ok\ntestLogTransform (__main__.Tests) ... ok\ntestMeanImputation (__main__.Tests) ... ok\ntestcleanText (__main__.Tests) ... ok\n\n----------------------------------------------------------------------\nRan 6 tests in 3.169s\n\nOK\nOut[6]: &lt;unittest.runner.TextTestResult run=6 errors=0 failures=0&gt;</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Final Project - QA","dashboards":[],"language":"python","widgets":{},"notebookOrigID":3225784875768127}},"nbformat":4,"nbformat_minor":0}
